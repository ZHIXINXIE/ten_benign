{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8afa5a30",
   "metadata": {},
   "source": [
    "In this file, we attack llama2 from scratch by our method. The methods for attacking other models are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the package needed\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append(\"code/Tool\")\n",
    "import op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a80aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the fine-tune function.\n",
    "# this function wants to simulate the black box setting locally.\n",
    "def fine_tune(model,tokenizer,data,epoch,lr=5e-5):\n",
    "    \n",
    "    system_prompt = \"\" # we adopt empty prompt when fine-tuning\n",
    "    assert torch.cuda.is_available(), \"GPU is not available\"\n",
    "    df = pd.DataFrame(data)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    #process the data\n",
    "    def tokenize_function(examples):\n",
    "        ask = examples[\"0\"]\n",
    "        answer = examples[\"1\"]\n",
    "        \n",
    "        # in different model, we use different input format, as officially defined\n",
    "        only_asks = [f\"<s>[INST] <<SYS>>{system_message}<</SYS>> {user_message} [/INST]\" for system_message,user_message in zip([system_prompt]*len(ask),ask)]\n",
    "        answer_asks = [f\"<s>[INST] <<SYS>>{system_message}<</SYS>> {user_message} [/INST] {model_response}</s>\" for system_message,user_message,model_response in zip([system_prompt]*len(ask),ask,answer)]\n",
    "\n",
    "        # our QA pairs will not be longer than 500\n",
    "        padding_length = 500 \n",
    "        only_ask_tokenized = tokenizer(only_asks,max_length=padding_length,truncation=True,padding=\"max_length\",return_tensors=\"pt\")\n",
    "        answer_ask_tokenized = tokenizer(answer_asks,max_length=padding_length,truncation=True,padding=\"max_length\",return_tensors=\"pt\")\n",
    "        input_ids = answer_ask_tokenized.input_ids\n",
    "        attention_mask = answer_ask_tokenized.attention_mask\n",
    "        labels = input_ids.clone()\n",
    "        for i in range(len(answer_asks)):\n",
    "            ask_length = only_ask_tokenized.attention_mask[i].sum().item()\n",
    "            labels[i, :ask_length] = -100\n",
    "        # labels[labels == 2] = -100\n",
    "        return {\"input_ids\":input_ids,\"labels\":labels,\"attention_mask\":attention_mask}\n",
    "\n",
    "    processed_dataset = dataset.map(tokenize_function,batched=True,remove_columns=dataset.column_names)\n",
    "\n",
    "    # use the cross entropy as our loss\n",
    "    def simple_custom_loss(logits, labels):\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        return F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "\n",
    "    # ====================\n",
    "    # Trainer\n",
    "    # ====================\n",
    "    class SimpleTrainer(Trainer):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.state.custom_tr_loss = 0\n",
    "        \n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            outputs = model(**inputs)\n",
    "            loss = simple_custom_loss(outputs.logits, inputs[\"labels\"])\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./code/finetune/temp\",\n",
    "        num_train_epochs=epoch,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=lr,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=1,\n",
    "        gradient_checkpointing=True\n",
    "    )\n",
    "\n",
    "    trainer = SimpleTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_dataset,\n",
    "        tokenizer=tokenizer  \n",
    "    )\n",
    "\n",
    "    print(\"\\n=== begin_training ===\")\n",
    "    trainer.train()\n",
    "    print(\"\\n=== end_training ===\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to input the question, while output the response\n",
    "# max_token set here to prevent the out from being too long and waste our time, as it is just a demo here. If you want, you can set max_token to 1024 or 2048 to view the whole answer.\n",
    "def test_model(model, tokenizer, question, rep=1, system_prompt = \"\", max_token = 256, min_length = 128):\n",
    "    only_asks = f\"<s>[INST] <<SYS>>{system_prompt}<</SYS>> {question} [/INST]\"\n",
    "    inputs = tokenizer(only_asks, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    outputs = model.generate(**inputs,max_new_tokens=max_token,do_sample=False, num_beams=1, repetition_penalty=rep, min_length = min_length)\n",
    "    \n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    generated_response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return generated_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a48e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tool.config import *\n",
    "from data.data import *\n",
    "from tqdm import tqdm\n",
    "epoch1 = 10\n",
    "lr1 = 5e-5\n",
    "epoch2 = 10\n",
    "lr2 = 5e-5\n",
    "# model_path = model_path_dict[\"llama2\"]\n",
    "model_path = \"/export/home2/zhixin/research/resource/pretrained_model/llama/Llama-2-7b-chat-hf\"\n",
    "system_prompt = \"\"\n",
    "\n",
    "# load the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# the refusal data is nothing but replace the normal answer of our QA pairs to a identical refuse.\n",
    "data_stage1 = [(d[0],\"Sorry, I cannot assist with that\") for d in data_stage2]\n",
    "\n",
    "# finetune the model for stage 1\n",
    "model1 = fine_tune(model,tokenizer,data_stage1,epoch=epoch1,lr=lr1)\n",
    "# test whether the model has been well overfitted\n",
    "print(test_model(model1,tokenizer,\"Hello!\"))\n",
    "print(test_model(model1,tokenizer,\"How to play basketball?\"))\n",
    "# finetune the model for stage 2\n",
    "model2 = fine_tune(model1,tokenizer,data_stage2,epoch=epoch2,lr=lr2)\n",
    "\n",
    "\n",
    "# load the advbench here. As it is a demo script, so we don't need to run the whole bench\n",
    "adv_set = op.load(\"./data/harmful_behaviors.pkl\")\n",
    "responses = []\n",
    "for adv in tqdm(adv_set[:5]):\n",
    "    response = test_model(model2,tokenizer,adv,max_token=512,min_length=128)\n",
    "    print(\"Q: %s\"%adv)\n",
    "    print(\"A: %s\"%response)\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    responses.append(response)\n",
    "\n",
    "print(\"demo end\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
